{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get theta data\n",
    "\n",
    "with open('theta.tsv','r', encoding='utf-8') as f_in:\n",
    "    theta_data = f_in.read()\n",
    "\n",
    "# very hacky! should be cleaned in pre-training textual data (if isn't already)\n",
    "theta_data = theta_data.replace('*','')\n",
    "theta_data = theta_data.replace('=','')\n",
    "\n",
    "theta_rows = theta_data.split('\\n')\n",
    "theta_rows.pop(-1); # blank final row\n",
    "theta_rows.pop(0); # header row with topic abbreviations\n",
    "theta_rows.pop(0); # useless \"!ctsdata\" second header row\n",
    "\n",
    "# print(\"first theta data row (100-char preview):\\n%s\\n\" % theta_rows[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_flt(s): return float(s) if s else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handy general function for getting max values of list in descending order\n",
    "def indices_of_top_N_elements(L, N):\n",
    "    return sorted(range(len(L)), key=lambda x: L[x], reverse=True)[:N]\n",
    "\n",
    "# characterize document by (pythonic index!) numbers (INT) for max_N topics over threshold and corresponding percentage (STRING)\n",
    "# for topic plot caption\n",
    "def get_top_topic_indices(doc_id, doc_thetas, max_N=5, threshold=3):\n",
    "# return list of tuples of type (%d, %s)\n",
    "    indices_of_dominant_N_topics = indices_of_top_N_elements(L=doc_thetas, N=max_N)\n",
    "    qualifying_indices = [  i\n",
    "                            for i in indices_of_dominant_N_topics\n",
    "                            if doc_thetas[i] >= threshold\n",
    "                            ]\n",
    "    return qualifying_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my solution for adjusting overlapping labels\n",
    "\n",
    "def update_box_position_values(pos_x, pos_y, label):\n",
    "    right, left = pos_x+len(label)/4, pos_x-len(label)/4\n",
    "    bottom, top = pos_y, pos_y+8\n",
    "    return right, left, bottom, top\n",
    "\n",
    "def anticipate_obstruction(interrupter_points, left, bottom, top):\n",
    "    for (x,y) in interrupter_points:\n",
    "        if x > left and (y >= bottom and y <= top):\n",
    "            return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8911 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MīmBh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1736/8911 [07:17<27:10,  4.40it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1759/8911 [07:23<24:54,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2394/8911 [10:09<23:29,  4.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NyKand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3266/8911 [13:53<21:57,  4.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriṃśBh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3400/8911 [14:29<33:54,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ŚVK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5197/8911 [22:24<14:03,  4.40it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 6582/8911 [29:33<09:05,  4.27it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViṃśV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 6618/8911 [29:41<08:43,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 7363/8911 [32:41<06:25,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SŚP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 7530/8911 [33:19<04:51,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VyV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8911/8911 [40:50<00:00,  3.64it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set plot size\n",
    "\n",
    "size_x, size_y = 1000, 700\n",
    "my_dpi=150\n",
    "\n",
    "generate_plots = True\n",
    "curr_text_name = ''\n",
    "if generate_plots:\n",
    "    for row in tqdm(theta_rows[19470:]):\n",
    "\n",
    "        cols = row.split('\\t')\n",
    "\n",
    "        plt.clf()\n",
    "        fig = plt.figure( figsize=(size_x/my_dpi, size_y/my_dpi), dpi=my_dpi )\n",
    "\n",
    "        doc_id = cols[1]\n",
    "\n",
    "        # for keeping track of output progress\n",
    "        text_name = doc_id[:doc_id.find('_')]\n",
    "        if text_name != curr_text_name:\n",
    "            print(text_name)\n",
    "            curr_text_name = text_name\n",
    "#         plt.title(\"document: \" + doc_id)\n",
    "        \n",
    "        plt.ylabel('% of doc ' + doc_id)\n",
    "        plt.xlabel('topic #')\n",
    "\n",
    "        axes = plt.gca()\n",
    "        ymin,ymax = 0,100\n",
    "        axes.set_ylim([ymin,ymax])\n",
    "\n",
    "        fig.patch.set_facecolor('white')\n",
    "\n",
    "        topic_nums = range(1,76)\n",
    "\n",
    "        doc_thetas_str_list = cols[3:]\n",
    "        doc_thetas = [ mk_flt(val)*100 for val in doc_thetas_str_list ]\n",
    "\n",
    "        plt.bar(topic_nums, doc_thetas)\n",
    "\n",
    "        dominant_topic_indices = get_top_topic_indices(doc_id, doc_thetas)\n",
    "        \n",
    "        interrupter_points = []\n",
    "\n",
    "        for container in axes.containers:\n",
    "\n",
    "            for i, rect in enumerate(container.get_children()):\n",
    "\n",
    "                if i not in dominant_topic_indices: continue\n",
    "                \n",
    "                height = rect.get_height()\n",
    "                width = rect.get_width()\n",
    "                x,y = rect.get_xy()\n",
    "                pos_x = x + width/2\n",
    "                pos_y = height+1\n",
    "                label = '#{}'.format(i+1) + '\\n({:.1f}%)'.format(height)\n",
    "                right, left, bottom, top = update_box_position_values(pos_x, pos_y, label)\n",
    "\n",
    "                while anticipate_obstruction(interrupter_points, left, bottom, top) == True:\n",
    "                    pos_y += 4\n",
    "                    bottom, top = pos_y, pos_y+8\n",
    "                    right, left, bottom, top = update_box_position_values(pos_x, pos_y, label)\n",
    "                \n",
    "                axes.annotate(label, (pos_x, pos_y), ha='center', va='bottom')\n",
    "                \n",
    "                interrupter_points += [(right,bottom), (right, top)]\n",
    "        \n",
    "        plt.savefig('png/' + doc_id + \".png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hereafter is manually filtering out words to improve phi interpretation\n",
    "# (from old explore_topic_top_words.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get phi data\n",
    "\n",
    "with open('phi.csv','r') as f_in:\n",
    "    phi_data = f_in.read()\n",
    "\n",
    "phi_data = phi_data.replace('\"','') # I think this here but not for theta because of way theta TSV was re-exported\n",
    "\n",
    "phi_rows = phi_data.split('\\n')\n",
    "phi_rows.pop(-1); # blank final row\n",
    "\n",
    "# print(\"first phi data row (100-char preview):\\n%s\\n\" % phi_rows[0][:100])\n",
    "# print(\"second phi data row (100-char preview):\\n%s\\n\" % phi_rows[1][:100])\n",
    "\n",
    "naive_topic_labels = phi_rows[0].split(',')[1:]\n",
    "K = len(naive_topic_labels)\n",
    "\n",
    "phis = OrderedDict()\n",
    "for row in phi_rows[1:]:\n",
    "    cells = row.split(',')\n",
    "    word, phi_values = cells[0], cells[1:]\n",
    "    phis[word] = [ float(ph) for ph in phi_values ]\n",
    "vocab = list(phis.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28381/28381 [00:00<00:00, 140940.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# prep true lexicon stats from theta data\n",
    "doc_fulltext = []\n",
    "for row in tqdm(theta_rows):\n",
    "    cols = row.split('\\t')\n",
    "    doc_fulltext.append(cols[2])\n",
    "\n",
    "corpus_string = ' '.join( doc_fulltext )\n",
    "corpus_string.replace('  ',' ');\n",
    "corpus_tokens = corpus_string.split()\n",
    "\n",
    "num_tokens = len(corpus_tokens)\n",
    "freq_w = Counter(corpus_tokens)\n",
    "def prob(w): return (freq_w[w] / num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance factor lambda by which phi values will be adjusted during consideration\n",
    "L = 0.8\n",
    "\n",
    "# 82 additional words to be filtered out during consideration (not used as stopwords in topic modeling)\n",
    "unwanted_words = ['a', 'sva', 'tvam', 'tathā', 'syāt', 'evam', 'āha', 'tva', 'tatra', 'asti', 'yadi', 'kim', 'tasya', 'yathā', 'sa', 'ced', 'yat', 'atas', 'etat', 'katham', 'ayam', 'bhavati', 'atra', 'tasmāt', 'vat', 'tā', 'uktam', 'tatas', 'atha', 'tvena', 'tve', 'asya', 'nanu', 'punar', 'idam', 'tadā', 'ucyate', 'tena', 'tayā', 'tāvat', 'yaḥ', 'sati', 'saḥ', 'sā', 'ādeḥ', 'tarhi', 'ādīnām', 'iva', 'ityādi', 'anena', 'ādayaḥ', 'kutas', 'yatas', 'te', 'iha', 'kaḥ', 'asau', 'kvacid', 'ādau', 'teṣām', 'yatra', 'kaścid', 'yena', 'ādiṣu', 'yasya', 'yadā', 'iyam', 'ukta', 'khalu', 'tām', 'tvasya', 'kiñcid', 'ādikam', 'astu', 'bhavet', 'eṣa', 'ete', 'kintu', 'tam', 'tayoḥ', 'yasmāt', 'ye']\n",
    "\n",
    "# depth of how many words to consider showing and how many to actually show for each topic during consideration\n",
    "max_consider = 500\n",
    "max_show = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust for L, find new top words\n",
    "# i.e., set the LDAvis slider\n",
    "\n",
    "class Topic:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.naive_label = \"\" # top_7_words\n",
    "        self.adjusted_phis = OrderedDict()\n",
    "        self.top_words = []\n",
    "        self.filtered_adjusted_phis = OrderedDict()\n",
    "\n",
    "Ts = []\n",
    "for i in range(K):\n",
    "\n",
    "    T = Topic()\n",
    "    T.num = i + 1\n",
    "    T.naive_label = naive_topic_labels[i]\n",
    "    T.adjusted_phis = { word: (L) * log(phis[word][i]) + (1 - L) * log(phis[word][i] / prob(word)) for word in vocab }\n",
    "    sorted_results = sorted(T.adjusted_phis.items(), key=lambda item: item[1], reverse=True)\n",
    "    sorted_relevance_dict = { res[0]: res[1] for res in sorted_results }\n",
    "    T.top_words = list(sorted_relevance_dict.keys())[:max_consider] # consider only max_consider words for each topic\n",
    "\n",
    "    Ts.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old phi:  0.00584789242497561\n",
      "prob:  0.0061816347958662645\n",
      "manual adjusted phi:  -13.252594575196175\n",
      "stored adjusted phi:  -13.252594575196175\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "i=3\n",
    "word='artham'\n",
    "print(\"old phi: \", phis['artha'][i])\n",
    "print(\"prob: \", prob('artha'))\n",
    "print(\"manual adjusted phi: \", (L) * log(phis[word][i]) + (1 - L) * log(phis[word][i] / prob(word)))\n",
    "\n",
    "print(\"stored adjusted phi: \", Ts[i].adjusted_phis[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n"
     ]
    }
   ],
   "source": [
    "print([len(T.top_words) for T in Ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unwanted words, retain only max_show words for each topic\n",
    "\n",
    "for T in Ts:\n",
    "    filtered_top_words = []\n",
    "    for t_w in T.top_words:\n",
    "        if t_w in unwanted_words: continue\n",
    "        filtered_top_words.append(t_w)\n",
    "    T.top_words = filtered_top_words[:max_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "300\n",
      "[300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]\n"
     ]
    }
   ],
   "source": [
    "print(len(Ts))\n",
    "print(max_show)\n",
    "print([len(T.top_words) for T in Ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output \n",
    "# for this typically want smaller max_words\n",
    "# (or could use larger and then delete manually)\n",
    "\n",
    "with open('top_words_%d_%d_%.1f.txt' % (max_consider, max_show, L), 'w') as f_out:\n",
    "    f_out.write( '\\n'.join([ str(T.num) + '\\t' + ' '.join(T.top_words) for T in Ts ]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in Ts:\n",
    "    T.filtered_adjusted_phis = { word: T.adjusted_phis[word] for word in T.top_words }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:41<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "#     regexp=r\"[a-zāīūṛṝḷṅñṭḍṇśṣḥṃ]+\",\n",
    "#     regexp=r\"[\\u1E00-\\u1EFF]+\",\n",
    "    font_path='HelveticaNeue.ttc',\n",
    "    max_words=500, \n",
    "    background_color=\"white\", \n",
    "    contour_color='steelblue', \n",
    "    contour_width=3, \n",
    "    width=800, \n",
    "    height=400\n",
    ")\n",
    "\n",
    "for i, T in enumerate(tqdm(Ts)):\n",
    "    wordcloud.generate_from_frequencies(frequencies=T.filtered_adjusted_phis)\n",
    "    wordcloud.to_file('cloud_pngs/topic_{:02}_wordcloud.png'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
